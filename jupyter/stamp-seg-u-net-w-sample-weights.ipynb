{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom imageio import imread\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import accuracy_score\nimport skimage\nfrom skimage.transform import *\nfrom skimage.color import *\nfrom skimage.filters import threshold_mean\nimport cv2\nimport keras\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.callbacks import *\nfrom keras.preprocessing.image import *\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport glob\npd.set_option('display.max_colwidth', -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f4884e7e3d98bf5290ce93730c3ea5d9e8fd440","collapsed":true},"cell_type":"code","source":"SCANS_DIR = \"../input/scans/scans/\"\nTRUTH_DIR = \"../input/ground-truth-pixel/ground-truth-pixel/\"\nIMG_SIZE=(2302,1632, 3)\nscan_files = glob.glob(SCANS_DIR+'*.png')\nscan_files = sorted(scan_files)\nscan_files_train = scan_files[:400]\nscan_files_test = scan_files[400:]\ntruth_files = glob.glob(TRUTH_DIR+'*.png')\ntruth_files = sorted(truth_files)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e3c48fa34ca90bb2119ba0347c109985a6dcbf8"},"cell_type":"markdown","source":"The stamps in the test images arture cartoon drawings, not texts, thus I selected a portion of the training images where the stamps are also drawings. The dataset is not randomized, so they concentrate in the last 40 or so training images."},{"metadata":{"trusted":true,"_uuid":"36be78f12ae1f1c78dd7ce8609dfe136a8f6241a"},"cell_type":"code","source":"scan_files_train_selected = scan_files_train[363:400]\ntruth_files_selected = truth_files[363:400]\npd.DataFrame({'scan':scan_files_train_selected, 'truth':truth_files_selected}).tail()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(100,10))\nfor i in range(10):\n    plt.subplot(1,10,i+1)\n    plt.imshow(imread(scan_files_train_selected[i]))\nplt.figure(figsize=(100,10))\nfor i in range(10):\n    plt.subplot(1,10,i+1)\n    plt.imshow(imread(scan_files_test[i]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e0be69af9bbbf8c5ed1b367748f69116d8368e7"},"cell_type":"markdown","source":"## Baseline Performance\n\nI use the majority class classifier as the baseline and take the first training image for example, the performance is as below."},{"metadata":{"trusted":true,"_uuid":"4a631ea2cc05f039fb47de56a6dbe46d3bb3a962"},"cell_type":"code","source":"y = 1-imread(truth_files_selected[0])[:,:,0].flatten()//255\ny_pred = np.zeros(y.shape)\n\nprint('accuracy_score=', accuracy_score(y,y_pred))\nprint('log_loss=', log_loss(y,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf9737749eac99728cae53ecef9af1c214aef9c4"},"cell_type":"markdown","source":"Because of the extreme bias, weights should be used. Instead of assigning class weights, I created a weight mask, so that pixels on or around the stamps have higheest weights, those around common texts and logos medium weights, and other the lowest weights, hoping that the network can learn to distinguish foreground from background, and further stamps from common texts."},{"metadata":{"trusted":true,"_uuid":"b014b49dd2378de2525ff035e1e3a1c2a7c5d9b7"},"cell_type":"code","source":"def calc_sample_weight(label, image):\n    image = rgb2gray(image)\n    around_texts = cv2.dilate((image < 0.5).astype(np.uint8) | label, np.ones((2,2)))\n    combined = around_texts + label\n    class_counts = np.unique(combined, return_counts=True)[1]\n    class_weight = np.sum(class_counts)/class_counts * np.array([1,1,2])[:len(class_counts)]\n    #class_weight = class_weight / np.max(class_weight)\n    weights = np.vectorize(lambda x: class_weight[x])(combined).flatten()\n    return weights,class_counts,class_weight\n\ny = 1-imread(truth_files_selected[0])[:,:,0].astype(np.uint8)//255\nx = imread(scan_files_train_selected[0])\nweights,class_counts,class_weight = calc_sample_weight(y, x)\nplt.figure(figsize=(10,10))\nplt.pie(class_counts, labels=['bg','text','stamp'], autopct='%.2f')\nprint('CLASS_WEIGHTS[bg,text,stamp]=',class_weight)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"446de008e80fdd49f3e4ab373965133d7b6d8d26","collapsed":true},"cell_type":"code","source":"def image_generator(files, randomized=True,labels=None, include_weights=False,batch_size=1, augment=False, gamma_range=(1,1), jitter_range=0):\n    img_batch = []\n    label_batch = []\n    weight_batch = []\n    while True:\n        indices = range(len(files))\n        if randomized:\n            indices = np.random.randint(len(files), size=len(files))\n        for i in indices:\n            if len(img_batch) >= batch_size:\n                img_batch = []\n                label_batch = []\n                weight_batch = []\n            img = imread(files[i])\n            img = img / 255\n            img = cv2.copyMakeBorder(img,\n                                     (IMG_SIZE[0]-img.shape[0])//2,\n                                     (IMG_SIZE[0]-img.shape[0])-(IMG_SIZE[0]-img.shape[0])//2,\n                                     (IMG_SIZE[1]-img.shape[1])//2,\n                                     (IMG_SIZE[1]-img.shape[1])-(IMG_SIZE[1]-img.shape[1])//2,\n                                     cv2.BORDER_REFLECT)\n            label = None\n            if labels is not None:\n                label = imread(labels[i])\n                label = (255-label) / 255\n                label = label[:,:,0].astype(np.uint8)\n                label = cv2.copyMakeBorder(label,\n                                         (IMG_SIZE[0]-label.shape[0])//2,\n                                         (IMG_SIZE[0]-label.shape[0])-(IMG_SIZE[0]-label.shape[0])//2,\n                                         (IMG_SIZE[1]-label.shape[1])//2,\n                                         (IMG_SIZE[1]-label.shape[1])-(IMG_SIZE[1]-label.shape[1])//2,\n                                         cv2.BORDER_REFLECT)\n            if augment:\n                gamma = np.random.uniform(gamma_range[0], gamma_range[1])\n                img = img**np.random.uniform(gamma_range[0], gamma_range[1])\n                \n                jitter = np.random.uniform(0, jitter_range, (4,2)).astype(np.float32)\n                pts1 = np.array(((0,0),(IMG_SIZE[1],0),(0,IMG_SIZE[0]),(IMG_SIZE[1],IMG_SIZE[0]))).astype(np.float32)\n                pts2 = pts1 + jitter\n                M = cv2.getPerspectiveTransform(pts1,pts2)\n                img = cv2.warpPerspective(img,M,(IMG_SIZE[1],IMG_SIZE[0]),borderValue=(1,1,1))\n                if labels is not None:\n                    label = cv2.warpPerspective(label,M,(IMG_SIZE[1],IMG_SIZE[0]),flags=cv2.INTER_NEAREST, borderValue=0)\n\n            img_batch.append(img)\n            if labels is not None:\n                if include_weights:\n                    weight_batch.append(calc_sample_weight(label,img)[0])\n                label_batch.append(label.reshape((IMG_SIZE[1]*IMG_SIZE[0],1)))\n            if len(img_batch) >= batch_size:\n                if labels is not None:\n                    if include_weights:\n                        yield np.array(img_batch), np.array(label_batch), np.array(weight_batch)\n                    else:\n                        yield np.array(img_batch), np.array(label_batch)\n                else:\n                    yield np.array(img_batch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b85feea5eba4e9f719742eae10c470ef9e407e8f"},"cell_type":"code","source":"scans, labels, weights = next(image_generator(scan_files_train_selected, labels=truth_files_selected, include_weights=True, batch_size=1, augment=True, gamma_range=(0.8,0.8), jitter_range=50))\n\nplt.figure(figsize=(50,50))\nplt.subplot(1,3,1)\nplt.imshow(scans[0]);\nplt.subplot(1,3,2)\nplt.imshow(labels[0].reshape((IMG_SIZE[0],IMG_SIZE[1])));\nplt.subplot(1,3,3)\nplt.imshow(weights[0].reshape((IMG_SIZE[0],IMG_SIZE[1])));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07dc05341e9ba03ae074725b7ea4d545e768e261","collapsed":true},"cell_type":"code","source":"def make_model():\n    inp = Input(IMG_SIZE)\n    x = ZeroPadding2D(((1,1),(16,16)))(inp)\n    skips=[]\n    for n in [9,12,12]:\n        skips.append(x)\n        x = Conv2D(n, kernel_size=3,strides=2,activation='relu',padding='same')(x)\n        x = BatchNormalization()(x)\n    for n in [9,12]:\n        x = UpSampling2D(size=2)(x)\n        x = concatenate([x, skips.pop()])\n        x = Conv2DTranspose(n, kernel_size=3,strides=1,activation='relu',padding='same')(x)\n        x = BatchNormalization()(x)\n    x = UpSampling2D(size=2)(x)\n    x = concatenate([x, skips.pop()])\n    x = Conv2DTranspose(1, kernel_size=3,strides=1,activation='sigmoid',padding='same')(x)\n    x = Cropping2D(((1,1),(16,16)))(x)\n    x = Reshape((IMG_SIZE[0]*IMG_SIZE[1],1,))(x)\n    return Model(inp, x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e73efa86724e6318649b950cfe056ac71ee4149","scrolled":false,"collapsed":true},"cell_type":"code","source":"model = make_model()\nmodel.compile(optimizer='adam',loss='binary_crossentropy', metrics=['acc'], sample_weight_mode=\"temporal\")\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12a5e6d55e317f76d314d61d33a5f2f1df8635f1","scrolled":true,"collapsed":true},"cell_type":"code","source":"!rm *.hdf5\n\nbatch_size=1\nepochs=90\ncheckpoint_period=30\nmodel.fit_generator(image_generator(scan_files_train_selected, labels=truth_files_selected, include_weights=True, batch_size=1, augment=True, gamma_range=(0.8,1.25), jitter_range=50),\n                    steps_per_epoch=len(scan_files_train_selected)//batch_size,\n                    validation_data=image_generator(scan_files_train_selected, labels=truth_files_selected, include_weights=True, batch_size=1, augment=True, gamma_range=(0.8,1.25), jitter_range=50),\n                    validation_steps=3,\n                    max_queue_size=1,\n                    epochs=epochs,\n                    callbacks=[ModelCheckpoint('./stamp.weights.{epoch:04d}.hdf5', period=checkpoint_period, save_weights_only=True)],\n                    verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c61d44265743d61f08a6dacfb2464c9eaa7ac93e","collapsed":true},"cell_type":"code","source":"steps=len(scan_files_test)\ncheckpoints = sorted(glob.glob('*.hdf5'))\nrows = len(checkpoints)+1\n\nplt.figure(figsize=(steps*10, rows*10))\nfor i in range(steps):\n    plt.subplot(rows,steps,i+1)\n    plt.imshow(imread(scan_files_test[i]))\nfor i, c in enumerate(checkpoints):\n    model.load_weights(c)\n    predicted = model.predict_generator(image_generator(scan_files_test,randomized=False, labels=None, include_weights=False, batch_size=1, augment=False),\n                       steps=steps)\n    predicted = np.round(predicted).reshape((steps,IMG_SIZE[0],IMG_SIZE[1]))\n    for s in range(steps):\n        plt.subplot(rows,steps,i*steps+s+steps+1)\n        plt.imshow(predicted[s])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}